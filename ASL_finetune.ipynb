{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ff6ce29d-b278-4437-bc88-bbbb93ccb84b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dfoda\\anaconda3\\envs\\ASLRecognition\\Lib\\site-packages\\torchvision\\io\\image.py:13: UserWarning: Failed to load image Python extension: '[WinError 127] Не найдена указанная процедура'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import cv2\n",
    "import matplotlib\n",
    "import mediapipe as mp\n",
    "from matplotlib import pyplot as plt\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import torchvision.models as models\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "import sys\n",
    "import os\n",
    "import shutil\n",
    "import warnings\n",
    "from torch.optim import Adam\n",
    "import torchmetrics\n",
    "import time\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f7d21da9-2fbe-4ea3-85c6-64bf743e1345",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module='google.protobuf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6641c02-2b43-40e1-8a03-353a6c9d289b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "794a791c-52b4-4ae6-b8b8-481d3a2d1186",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_WIDTH = IMG_HEIGHT = SIZE = 100\n",
    "BATCH_SIZE = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e1070e40-ecfb-426b-955d-c58f4318cddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((IMG_HEIGHT, IMG_WIDTH)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5bf1da70-e27b-40da-90f9-45a9078fcfe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((IMG_HEIGHT, IMG_WIDTH)),\n",
    "    transforms.ToTensor()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "01a5d2da-609c-4aa7-8177-34e412b59459",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = ImageFolder('data/processed_images_hands_init/', transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0c81cdaf-56db-4ddc-bac3-e0032728f710",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(0.8 * len(dataset))\n",
    "valid_size = len(dataset) - train_size\n",
    "\n",
    "train_dataset, valid_dataset = random_split(dataset, (train_size, valid_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e4b3862a-6510-48cd-9f6e-0da0a33552bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2347a0c8-84a2-4507-9fa1-08ecf0c4f340",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c141c83-eb6d-45ce-b59d-d29666500a4c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ab57ab54-e7c5-4d10-a7a4-954bb848948c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_model(model, num_classes=29, lr=0.001, device=None):\n",
    "\n",
    "    num_features = model.fc.in_features\n",
    "    model.fc = nn.Linear(num_features, num_classes)\n",
    "    \n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    for param in model.fc.parameters():\n",
    "        param.requires_grad = True\n",
    "\n",
    "    optimizer = Adam(model.fc.parameters(), lr=lr)\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    if device is None:\n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(device)\n",
    "\n",
    "    accuracy = torchmetrics.Accuracy(task='MULTICLASS', num_classes=29)\n",
    "    accuracy.to(device)\n",
    "    \n",
    "    return model, criterion, optimizer, accuracy, device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0bab0962-34b9-4dbe-b669-3ea0bcb06a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    def __init__(self, patience=5, verbose=False):\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.val_loss_min = float('inf')\n",
    "\n",
    "    def __call__(self, val_loss, model):\n",
    "        score = -val_loss\n",
    "\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "        elif score < self.best_score:\n",
    "            self.counter += 1\n",
    "            if self.verbose:\n",
    "                print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "            self.counter = 0\n",
    "\n",
    "    def save_checkpoint(self, val_loss, model):\n",
    "        '''Saves model when validation loss decreases.'''\n",
    "        if self.verbose:\n",
    "            print(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n",
    "        torch.save(model.state_dict(), 'models/vgg16_best_model.pth')\n",
    "        self.val_loss_min = val_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b2fcca22-086f-4e9c-bf68-ef8ab506f565",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, num_epochs, train_loader, val_loader, optimizer, loss_fun, accuracy, device, early_stopping=False):\n",
    "    model.to(device)\n",
    "    print(model.__class__.__name__)\n",
    "    \n",
    "    if early_stopping:\n",
    "        early_stopping = EarlyStopping(patience=40, verbose=True)\n",
    "        \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        num_batches = 0\n",
    "        \n",
    "        accuracy.reset()  # Reset metric at the beginning of each epoch\n",
    "        \n",
    "        for images, labels in tqdm(train_loader, desc=f\"Training Epoch {epoch+1}/{num_epochs}\", total=len(train_loader)):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = loss_fun(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            accuracy.update(outputs, labels)\n",
    "            running_loss += loss.item()\n",
    "            num_batches += 1\n",
    "        \n",
    "        average_loss = running_loss / num_batches\n",
    "        epoch_accuracy = accuracy.compute()\n",
    "        \n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_batches = 0\n",
    "        val_accuracy = 0.0\n",
    "        with torch.no_grad():\n",
    "            for images, labels in tqdm(val_loader, desc=f\"Validation Epoch {epoch+1}/{num_epochs}\", total=len(val_loader)):\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                outputs = model(images)\n",
    "                loss = loss_fun(outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "                val_batches += 1\n",
    "                val_accuracy += torch.sum(torch.argmax(outputs, dim=1) == labels).item()\n",
    "        \n",
    "        average_val_loss = val_loss / val_batches\n",
    "        val_accuracy = val_accuracy / len(val_loader.dataset)\n",
    "        \n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {average_loss:.4f}, Accuracy: {epoch_accuracy:.4f}, Val Loss: {average_val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}\")\n",
    "\n",
    "        if early_stopping:\n",
    "            early_stopping(average_val_loss, model)\n",
    "            if early_stopping.early_stop:\n",
    "                print(\"Early stopping\")\n",
    "                break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "99b36466-f490-402a-97dd-a3dc64c64855",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dfoda\\anaconda3\\envs\\ASLRecognition\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\dfoda\\anaconda3\\envs\\ASLRecognition\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "resnet50 = models.resnet50(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c60083a4-94aa-4b6b-a849-0c6ec1260069",
   "metadata": {},
   "outputs": [],
   "source": [
    "model, criterion, optimizer, accuracy, device = prepare_model(resnet50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "0326d7c8-4367-44f6-aee1-8d61bf4cab70",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "8a6e9063-5f5b-4f14-b172-28afd8783ab9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1/20: 100%|███████████████████████████████████████████████████████████| 902/902 [06:24<00:00,  2.34it/s]\n",
      "Validation Epoch 1/20: 100%|████████████████████████████████████████████████████████▋| 225/226 [01:34<00:00,  2.47it/s]C:\\Users\\dfoda\\anaconda3\\envs\\ASLRecognition\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:456: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\cudnn\\Conv_v8.cpp:919.)\n",
      "  return F.conv2d(input, weight, bias, self.stride,\n",
      "Validation Epoch 1/20: 100%|█████████████████████████████████████████████████████████| 226/226 [01:35<00:00,  2.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20], Loss: 0.5252, Accuracy: 0.8715, Val Loss: 0.2479, Val Accuracy: 0.9299\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2/20: 100%|███████████████████████████████████████████████████████████| 902/902 [06:28<00:00,  2.32it/s]\n",
      "Validation Epoch 2/20: 100%|█████████████████████████████████████████████████████████| 226/226 [01:33<00:00,  2.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/20], Loss: 0.2072, Accuracy: 0.9435, Val Loss: 0.1836, Val Accuracy: 0.9435\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3/20: 100%|███████████████████████████████████████████████████████████| 902/902 [06:07<00:00,  2.45it/s]\n",
      "Validation Epoch 3/20: 100%|█████████████████████████████████████████████████████████| 226/226 [01:32<00:00,  2.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/20], Loss: 0.1561, Accuracy: 0.9560, Val Loss: 0.1271, Val Accuracy: 0.9636\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4/20: 100%|███████████████████████████████████████████████████████████| 902/902 [06:07<00:00,  2.45it/s]\n",
      "Validation Epoch 4/20: 100%|█████████████████████████████████████████████████████████| 226/226 [01:32<00:00,  2.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/20], Loss: 0.1255, Accuracy: 0.9647, Val Loss: 0.1214, Val Accuracy: 0.9657\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5/20: 100%|███████████████████████████████████████████████████████████| 902/902 [05:57<00:00,  2.52it/s]\n",
      "Validation Epoch 5/20: 100%|█████████████████████████████████████████████████████████| 226/226 [01:33<00:00,  2.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/20], Loss: 0.1125, Accuracy: 0.9678, Val Loss: 0.1223, Val Accuracy: 0.9632\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 6/20: 100%|███████████████████████████████████████████████████████████| 902/902 [05:46<00:00,  2.61it/s]\n",
      "Validation Epoch 6/20: 100%|█████████████████████████████████████████████████████████| 226/226 [01:31<00:00,  2.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/20], Loss: 0.1004, Accuracy: 0.9702, Val Loss: 0.1000, Val Accuracy: 0.9702\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 7/20: 100%|███████████████████████████████████████████████████████████| 902/902 [05:44<00:00,  2.62it/s]\n",
      "Validation Epoch 7/20: 100%|█████████████████████████████████████████████████████████| 226/226 [01:38<00:00,  2.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/20], Loss: 0.0898, Accuracy: 0.9733, Val Loss: 0.0917, Val Accuracy: 0.9725\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 8/20: 100%|███████████████████████████████████████████████████████████| 902/902 [06:35<00:00,  2.28it/s]\n",
      "Validation Epoch 8/20: 100%|█████████████████████████████████████████████████████████| 226/226 [01:41<00:00,  2.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/20], Loss: 0.0842, Accuracy: 0.9759, Val Loss: 0.0799, Val Accuracy: 0.9762\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 9/20: 100%|███████████████████████████████████████████████████████████| 902/902 [06:31<00:00,  2.30it/s]\n",
      "Validation Epoch 9/20: 100%|█████████████████████████████████████████████████████████| 226/226 [01:35<00:00,  2.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9/20], Loss: 0.0744, Accuracy: 0.9782, Val Loss: 0.0829, Val Accuracy: 0.9748\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 10/20: 100%|██████████████████████████████████████████████████████████| 902/902 [06:22<00:00,  2.36it/s]\n",
      "Validation Epoch 10/20: 100%|████████████████████████████████████████████████████████| 226/226 [01:38<00:00,  2.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/20], Loss: 0.0657, Accuracy: 0.9805, Val Loss: 0.0742, Val Accuracy: 0.9768\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 11/20: 100%|██████████████████████████████████████████████████████████| 902/902 [06:17<00:00,  2.39it/s]\n",
      "Validation Epoch 11/20: 100%|████████████████████████████████████████████████████████| 226/226 [01:34<00:00,  2.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [11/20], Loss: 0.0582, Accuracy: 0.9833, Val Loss: 0.0740, Val Accuracy: 0.9784\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 12/20: 100%|██████████████████████████████████████████████████████████| 902/902 [06:11<00:00,  2.43it/s]\n",
      "Validation Epoch 12/20: 100%|████████████████████████████████████████████████████████| 226/226 [01:32<00:00,  2.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [12/20], Loss: 0.0578, Accuracy: 0.9835, Val Loss: 0.0859, Val Accuracy: 0.9731\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 13/20: 100%|██████████████████████████████████████████████████████████| 902/902 [06:06<00:00,  2.46it/s]\n",
      "Validation Epoch 13/20: 100%|████████████████████████████████████████████████████████| 226/226 [01:37<00:00,  2.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [13/20], Loss: 0.0527, Accuracy: 0.9846, Val Loss: 0.0733, Val Accuracy: 0.9766\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 14/20: 100%|██████████████████████████████████████████████████████████| 902/902 [06:01<00:00,  2.49it/s]\n",
      "Validation Epoch 14/20: 100%|████████████████████████████████████████████████████████| 226/226 [01:33<00:00,  2.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [14/20], Loss: 0.0551, Accuracy: 0.9842, Val Loss: 0.0799, Val Accuracy: 0.9746\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 15/20: 100%|██████████████████████████████████████████████████████████| 902/902 [06:05<00:00,  2.47it/s]\n",
      "Validation Epoch 15/20: 100%|████████████████████████████████████████████████████████| 226/226 [01:35<00:00,  2.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [15/20], Loss: 0.0527, Accuracy: 0.9849, Val Loss: 0.0799, Val Accuracy: 0.9752\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 16/20: 100%|██████████████████████████████████████████████████████████| 902/902 [06:26<00:00,  2.33it/s]\n",
      "Validation Epoch 16/20: 100%|████████████████████████████████████████████████████████| 226/226 [01:33<00:00,  2.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [16/20], Loss: 0.0528, Accuracy: 0.9854, Val Loss: 0.0718, Val Accuracy: 0.9776\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 17/20: 100%|██████████████████████████████████████████████████████████| 902/902 [06:02<00:00,  2.49it/s]\n",
      "Validation Epoch 17/20: 100%|████████████████████████████████████████████████████████| 226/226 [01:35<00:00,  2.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [17/20], Loss: 0.0446, Accuracy: 0.9870, Val Loss: 0.0602, Val Accuracy: 0.9825\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 18/20: 100%|██████████████████████████████████████████████████████████| 902/902 [05:57<00:00,  2.52it/s]\n",
      "Validation Epoch 18/20: 100%|████████████████████████████████████████████████████████| 226/226 [01:38<00:00,  2.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [18/20], Loss: 0.0430, Accuracy: 0.9875, Val Loss: 0.0778, Val Accuracy: 0.9752\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 19/20: 100%|██████████████████████████████████████████████████████████| 902/902 [05:59<00:00,  2.51it/s]\n",
      "Validation Epoch 19/20: 100%|████████████████████████████████████████████████████████| 226/226 [01:32<00:00,  2.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [19/20], Loss: 0.0410, Accuracy: 0.9884, Val Loss: 0.0629, Val Accuracy: 0.9799\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 20/20: 100%|██████████████████████████████████████████████████████████| 902/902 [05:49<00:00,  2.58it/s]\n",
      "Validation Epoch 20/20: 100%|████████████████████████████████████████████████████████| 226/226 [01:31<00:00,  2.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [20/20], Loss: 0.0385, Accuracy: 0.9892, Val Loss: 0.0626, Val Accuracy: 0.9806\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "resnet50_trained = train_model(model, num_epochs, train_loader, valid_loader, optimizer, criterion, accuracy, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "9459e538-5226-46d9-ac0c-44a4cd907108",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'models/MobileNet_20.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "a1fe0182-bc36-4cb7-bee1-e77295ccc1ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_vgg16(model, num_classes=29, lr=0.001, device=None):\n",
    "    num_features = model.classifier[-1].in_features\n",
    "    model.classifier[-1] = nn.Linear(num_features, num_classes)\n",
    "\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    for param in model.classifier[-1].parameters():\n",
    "        param.requires_grad = True\n",
    "\n",
    "    optimizer = Adam(model.classifier[-1].parameters(), lr=lr)\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    if device is None:\n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    accuracy = torchmetrics.Accuracy(task='MULTICLASS', num_classes=29)\n",
    "    accuracy.to(device)\n",
    "    \n",
    "    return model, criterion, optimizer, accuracy, device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "48d90ca8-82d0-49a1-b3c8-58a8de325e87",
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg16 = models.vgg16(pretrained=True)\n",
    "model, criterion, optimizer, accuracy, device = prepare_vgg16(vgg16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "c9df63b3-0c1d-4ebc-ae68-6ed3f92d27ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VGG\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1/20: 100%|███████████████████████████████████████████████████████████| 902/902 [08:56<00:00,  1.68it/s]\n",
      "Validation Epoch 1/20: 100%|█████████████████████████████████████████████████████████| 226/226 [02:13<00:00,  1.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20], Loss: 0.4886, Accuracy: 0.8539, Val Loss: 0.1681, Val Accuracy: 0.9591\n",
      "Validation loss decreased (inf --> 0.168092).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2/20: 100%|███████████████████████████████████████████████████████████| 902/902 [08:46<00:00,  1.71it/s]\n",
      "Validation Epoch 2/20: 100%|█████████████████████████████████████████████████████████| 226/226 [02:12<00:00,  1.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/20], Loss: 0.2668, Accuracy: 0.9125, Val Loss: 0.1150, Val Accuracy: 0.9719\n",
      "Validation loss decreased (0.168092 --> 0.114986).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3/20: 100%|███████████████████████████████████████████████████████████| 902/902 [08:47<00:00,  1.71it/s]\n",
      "Validation Epoch 3/20: 100%|█████████████████████████████████████████████████████████| 226/226 [02:10<00:00,  1.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/20], Loss: 0.2425, Accuracy: 0.9189, Val Loss: 0.0865, Val Accuracy: 0.9779\n",
      "Validation loss decreased (0.114986 --> 0.086510).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4/20: 100%|███████████████████████████████████████████████████████████| 902/902 [08:38<00:00,  1.74it/s]\n",
      "Validation Epoch 4/20: 100%|█████████████████████████████████████████████████████████| 226/226 [02:12<00:00,  1.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/20], Loss: 0.2238, Accuracy: 0.9254, Val Loss: 0.0815, Val Accuracy: 0.9774\n",
      "Validation loss decreased (0.086510 --> 0.081471).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5/20: 100%|███████████████████████████████████████████████████████████| 902/902 [08:37<00:00,  1.74it/s]\n",
      "Validation Epoch 5/20: 100%|█████████████████████████████████████████████████████████| 226/226 [02:09<00:00,  1.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/20], Loss: 0.2217, Accuracy: 0.9268, Val Loss: 0.0726, Val Accuracy: 0.9797\n",
      "Validation loss decreased (0.081471 --> 0.072644).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 6/20: 100%|███████████████████████████████████████████████████████████| 902/902 [08:36<00:00,  1.75it/s]\n",
      "Validation Epoch 6/20: 100%|█████████████████████████████████████████████████████████| 226/226 [02:11<00:00,  1.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/20], Loss: 0.2119, Accuracy: 0.9295, Val Loss: 0.0639, Val Accuracy: 0.9822\n",
      "Validation loss decreased (0.072644 --> 0.063908).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 7/20: 100%|███████████████████████████████████████████████████████████| 902/902 [08:38<00:00,  1.74it/s]\n",
      "Validation Epoch 7/20: 100%|█████████████████████████████████████████████████████████| 226/226 [02:10<00:00,  1.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/20], Loss: 0.2092, Accuracy: 0.9307, Val Loss: 0.0619, Val Accuracy: 0.9833\n",
      "Validation loss decreased (0.063908 --> 0.061851).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 8/20: 100%|███████████████████████████████████████████████████████████| 902/902 [08:27<00:00,  1.78it/s]\n",
      "Validation Epoch 8/20: 100%|█████████████████████████████████████████████████████████| 226/226 [02:11<00:00,  1.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/20], Loss: 0.2086, Accuracy: 0.9324, Val Loss: 0.0633, Val Accuracy: 0.9822\n",
      "EarlyStopping counter: 1 out of 40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 9/20: 100%|███████████████████████████████████████████████████████████| 902/902 [08:14<00:00,  1.83it/s]\n",
      "Validation Epoch 9/20: 100%|█████████████████████████████████████████████████████████| 226/226 [02:08<00:00,  1.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9/20], Loss: 0.2044, Accuracy: 0.9330, Val Loss: 0.0578, Val Accuracy: 0.9830\n",
      "Validation loss decreased (0.061851 --> 0.057838).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 10/20: 100%|██████████████████████████████████████████████████████████| 902/902 [08:47<00:00,  1.71it/s]\n",
      "Validation Epoch 10/20: 100%|████████████████████████████████████████████████████████| 226/226 [02:10<00:00,  1.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/20], Loss: 0.2015, Accuracy: 0.9349, Val Loss: 0.0575, Val Accuracy: 0.9830\n",
      "Validation loss decreased (0.057838 --> 0.057547).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 11/20: 100%|██████████████████████████████████████████████████████████| 902/902 [08:30<00:00,  1.77it/s]\n",
      "Validation Epoch 11/20: 100%|████████████████████████████████████████████████████████| 226/226 [02:09<00:00,  1.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [11/20], Loss: 0.1963, Accuracy: 0.9375, Val Loss: 0.0512, Val Accuracy: 0.9859\n",
      "Validation loss decreased (0.057547 --> 0.051178).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 12/20: 100%|██████████████████████████████████████████████████████████| 902/902 [08:33<00:00,  1.76it/s]\n",
      "Validation Epoch 12/20: 100%|████████████████████████████████████████████████████████| 226/226 [02:10<00:00,  1.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [12/20], Loss: 0.1989, Accuracy: 0.9373, Val Loss: 0.0535, Val Accuracy: 0.9845\n",
      "EarlyStopping counter: 1 out of 40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 13/20: 100%|██████████████████████████████████████████████████████████| 902/902 [08:17<00:00,  1.81it/s]\n",
      "Validation Epoch 13/20: 100%|████████████████████████████████████████████████████████| 226/226 [02:11<00:00,  1.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [13/20], Loss: 0.1915, Accuracy: 0.9401, Val Loss: 0.0588, Val Accuracy: 0.9815\n",
      "EarlyStopping counter: 2 out of 40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 14/20: 100%|██████████████████████████████████████████████████████████| 902/902 [08:14<00:00,  1.83it/s]\n",
      "Validation Epoch 14/20: 100%|████████████████████████████████████████████████████████| 226/226 [02:10<00:00,  1.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [14/20], Loss: 0.1982, Accuracy: 0.9386, Val Loss: 0.0512, Val Accuracy: 0.9843\n",
      "EarlyStopping counter: 3 out of 40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 15/20: 100%|██████████████████████████████████████████████████████████| 902/902 [08:16<00:00,  1.82it/s]\n",
      "Validation Epoch 15/20: 100%|████████████████████████████████████████████████████████| 226/226 [02:12<00:00,  1.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [15/20], Loss: 0.1903, Accuracy: 0.9407, Val Loss: 0.0469, Val Accuracy: 0.9858\n",
      "Validation loss decreased (0.051178 --> 0.046871).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 16/20: 100%|██████████████████████████████████████████████████████████| 902/902 [09:10<00:00,  1.64it/s]\n",
      "Validation Epoch 16/20: 100%|████████████████████████████████████████████████████████| 226/226 [02:16<00:00,  1.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [16/20], Loss: 0.1973, Accuracy: 0.9392, Val Loss: 0.0453, Val Accuracy: 0.9873\n",
      "Validation loss decreased (0.046871 --> 0.045271).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 17/20: 100%|██████████████████████████████████████████████████████████| 902/902 [09:06<00:00,  1.65it/s]\n",
      "Validation Epoch 17/20: 100%|████████████████████████████████████████████████████████| 226/226 [02:15<00:00,  1.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [17/20], Loss: 0.1997, Accuracy: 0.9381, Val Loss: 0.0463, Val Accuracy: 0.9879\n",
      "EarlyStopping counter: 1 out of 40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 18/20: 100%|██████████████████████████████████████████████████████████| 902/902 [09:12<00:00,  1.63it/s]\n",
      "Validation Epoch 18/20: 100%|████████████████████████████████████████████████████████| 226/226 [02:17<00:00,  1.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [18/20], Loss: 0.1927, Accuracy: 0.9412, Val Loss: 0.0396, Val Accuracy: 0.9890\n",
      "Validation loss decreased (0.045271 --> 0.039566).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 19/20: 100%|██████████████████████████████████████████████████████████| 902/902 [09:29<00:00,  1.58it/s]\n",
      "Validation Epoch 19/20: 100%|████████████████████████████████████████████████████████| 226/226 [02:16<00:00,  1.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [19/20], Loss: 0.1899, Accuracy: 0.9412, Val Loss: 0.0501, Val Accuracy: 0.9847\n",
      "EarlyStopping counter: 1 out of 40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 20/20: 100%|██████████████████████████████████████████████████████████| 902/902 [08:52<00:00,  1.69it/s]\n",
      "Validation Epoch 20/20: 100%|████████████████████████████████████████████████████████| 226/226 [02:14<00:00,  1.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [20/20], Loss: 0.1895, Accuracy: 0.9415, Val Loss: 0.0490, Val Accuracy: 0.9877\n",
      "EarlyStopping counter: 2 out of 40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_model(model, num_epochs, train_loader, valid_loader, optimizer, criterion, accuracy, device, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "8bd2823a-6cd0-4ed1-bb2f-132e458a3f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_mobilenetv2(model, num_classes=29, lr=0.001, device=None):\n",
    "    num_features = model.classifier[1].in_features\n",
    "    model.classifier[1] = nn.Linear(num_features, num_classes)\n",
    "\n",
    "    for param in model.features.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    optimizer = Adam(model.classifier.parameters(), lr=lr)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    if device is None:\n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    accuracy = torchmetrics.Accuracy(task='MULTICLASS', num_classes=29)\n",
    "    accuracy.to(device)\n",
    "    \n",
    "    return model, criterion, optimizer, accuracy, device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f9a4eb3f-9f28-47f3-8a53-ec8bd09aee9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "mobilenet = models.mobilenet_v2(pretrained=True)\n",
    "model, criterion, optimizer, accuracy, device = prepare_mobilenetv2(mobilenet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "98454f45-7eff-483b-a23b-0c26667ecd2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MobileNetV2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1/20: 100%|███████████████████████████████████████████████████████████| 902/902 [05:39<00:00,  2.66it/s]\n",
      "Validation Epoch 1/20: 100%|█████████████████████████████████████████████████████████| 226/226 [01:28<00:00,  2.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20], Loss: 0.4978, Accuracy: 0.8787, Val Loss: 0.1808, Val Accuracy: 0.9587\n",
      "Validation loss decreased (inf --> 0.180827).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2/20: 100%|███████████████████████████████████████████████████████████| 902/902 [05:29<00:00,  2.73it/s]\n",
      "Validation Epoch 2/20: 100%|█████████████████████████████████████████████████████████| 226/226 [01:30<00:00,  2.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/20], Loss: 0.1989, Accuracy: 0.9442, Val Loss: 0.1392, Val Accuracy: 0.9633\n",
      "Validation loss decreased (0.180827 --> 0.139204).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3/20: 100%|███████████████████████████████████████████████████████████| 902/902 [05:37<00:00,  2.67it/s]\n",
      "Validation Epoch 3/20: 100%|█████████████████████████████████████████████████████████| 226/226 [01:23<00:00,  2.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/20], Loss: 0.1629, Accuracy: 0.9514, Val Loss: 0.1030, Val Accuracy: 0.9722\n",
      "Validation loss decreased (0.139204 --> 0.102962).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4/20: 100%|███████████████████████████████████████████████████████████| 902/902 [05:42<00:00,  2.63it/s]\n",
      "Validation Epoch 4/20: 100%|█████████████████████████████████████████████████████████| 226/226 [01:25<00:00,  2.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/20], Loss: 0.1411, Accuracy: 0.9566, Val Loss: 0.0929, Val Accuracy: 0.9756\n",
      "Validation loss decreased (0.102962 --> 0.092850).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5/20: 100%|███████████████████████████████████████████████████████████| 902/902 [05:36<00:00,  2.68it/s]\n",
      "Validation Epoch 5/20: 100%|█████████████████████████████████████████████████████████| 226/226 [01:31<00:00,  2.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/20], Loss: 0.1306, Accuracy: 0.9586, Val Loss: 0.0829, Val Accuracy: 0.9758\n",
      "Validation loss decreased (0.092850 --> 0.082890).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 6/20: 100%|███████████████████████████████████████████████████████████| 902/902 [05:48<00:00,  2.59it/s]\n",
      "Validation Epoch 6/20: 100%|█████████████████████████████████████████████████████████| 226/226 [01:25<00:00,  2.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/20], Loss: 0.1252, Accuracy: 0.9594, Val Loss: 0.0805, Val Accuracy: 0.9754\n",
      "Validation loss decreased (0.082890 --> 0.080547).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 7/20: 100%|███████████████████████████████████████████████████████████| 902/902 [05:18<00:00,  2.83it/s]\n",
      "Validation Epoch 7/20: 100%|█████████████████████████████████████████████████████████| 226/226 [01:24<00:00,  2.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/20], Loss: 0.1212, Accuracy: 0.9609, Val Loss: 0.0847, Val Accuracy: 0.9746\n",
      "EarlyStopping counter: 1 out of 40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 8/20: 100%|███████████████████████████████████████████████████████████| 902/902 [05:45<00:00,  2.61it/s]\n",
      "Validation Epoch 8/20: 100%|█████████████████████████████████████████████████████████| 226/226 [01:23<00:00,  2.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/20], Loss: 0.1159, Accuracy: 0.9621, Val Loss: 0.0775, Val Accuracy: 0.9778\n",
      "Validation loss decreased (0.080547 --> 0.077483).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 9/20: 100%|███████████████████████████████████████████████████████████| 902/902 [05:18<00:00,  2.84it/s]\n",
      "Validation Epoch 9/20: 100%|█████████████████████████████████████████████████████████| 226/226 [01:21<00:00,  2.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9/20], Loss: 0.1090, Accuracy: 0.9630, Val Loss: 0.0744, Val Accuracy: 0.9777\n",
      "Validation loss decreased (0.077483 --> 0.074443).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 10/20: 100%|██████████████████████████████████████████████████████████| 902/902 [05:15<00:00,  2.86it/s]\n",
      "Validation Epoch 10/20: 100%|████████████████████████████████████████████████████████| 226/226 [01:28<00:00,  2.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/20], Loss: 0.1117, Accuracy: 0.9631, Val Loss: 0.0762, Val Accuracy: 0.9773\n",
      "EarlyStopping counter: 1 out of 40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 11/20: 100%|██████████████████████████████████████████████████████████| 902/902 [05:22<00:00,  2.79it/s]\n",
      "Validation Epoch 11/20: 100%|████████████████████████████████████████████████████████| 226/226 [01:20<00:00,  2.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [11/20], Loss: 0.1124, Accuracy: 0.9643, Val Loss: 0.0669, Val Accuracy: 0.9799\n",
      "Validation loss decreased (0.074443 --> 0.066865).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 12/20: 100%|██████████████████████████████████████████████████████████| 902/902 [05:24<00:00,  2.78it/s]\n",
      "Validation Epoch 12/20: 100%|████████████████████████████████████████████████████████| 226/226 [01:24<00:00,  2.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [12/20], Loss: 0.1118, Accuracy: 0.9639, Val Loss: 0.0803, Val Accuracy: 0.9767\n",
      "EarlyStopping counter: 1 out of 40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 13/20: 100%|██████████████████████████████████████████████████████████| 902/902 [05:36<00:00,  2.68it/s]\n",
      "Validation Epoch 13/20: 100%|████████████████████████████████████████████████████████| 226/226 [01:22<00:00,  2.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [13/20], Loss: 0.1077, Accuracy: 0.9650, Val Loss: 0.0673, Val Accuracy: 0.9819\n",
      "EarlyStopping counter: 2 out of 40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 14/20: 100%|██████████████████████████████████████████████████████████| 902/902 [05:31<00:00,  2.72it/s]\n",
      "Validation Epoch 14/20: 100%|████████████████████████████████████████████████████████| 226/226 [01:25<00:00,  2.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [14/20], Loss: 0.1082, Accuracy: 0.9662, Val Loss: 0.0644, Val Accuracy: 0.9819\n",
      "Validation loss decreased (0.066865 --> 0.064393).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 15/20: 100%|██████████████████████████████████████████████████████████| 902/902 [05:31<00:00,  2.72it/s]\n",
      "Validation Epoch 15/20: 100%|████████████████████████████████████████████████████████| 226/226 [01:20<00:00,  2.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [15/20], Loss: 0.1064, Accuracy: 0.9646, Val Loss: 0.0671, Val Accuracy: 0.9813\n",
      "EarlyStopping counter: 1 out of 40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 16/20: 100%|██████████████████████████████████████████████████████████| 902/902 [05:20<00:00,  2.81it/s]\n",
      "Validation Epoch 16/20: 100%|████████████████████████████████████████████████████████| 226/226 [01:23<00:00,  2.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [16/20], Loss: 0.1019, Accuracy: 0.9660, Val Loss: 0.0690, Val Accuracy: 0.9799\n",
      "EarlyStopping counter: 2 out of 40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 17/20: 100%|██████████████████████████████████████████████████████████| 902/902 [05:38<00:00,  2.67it/s]\n",
      "Validation Epoch 17/20: 100%|████████████████████████████████████████████████████████| 226/226 [01:22<00:00,  2.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [17/20], Loss: 0.1049, Accuracy: 0.9665, Val Loss: 0.0625, Val Accuracy: 0.9818\n",
      "Validation loss decreased (0.064393 --> 0.062538).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 18/20: 100%|██████████████████████████████████████████████████████████| 902/902 [05:39<00:00,  2.66it/s]\n",
      "Validation Epoch 18/20: 100%|████████████████████████████████████████████████████████| 226/226 [01:18<00:00,  2.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [18/20], Loss: 0.1041, Accuracy: 0.9657, Val Loss: 0.0710, Val Accuracy: 0.9800\n",
      "EarlyStopping counter: 1 out of 40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 19/20: 100%|██████████████████████████████████████████████████████████| 902/902 [05:25<00:00,  2.77it/s]\n",
      "Validation Epoch 19/20: 100%|████████████████████████████████████████████████████████| 226/226 [01:23<00:00,  2.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [19/20], Loss: 0.1048, Accuracy: 0.9651, Val Loss: 0.0757, Val Accuracy: 0.9807\n",
      "EarlyStopping counter: 2 out of 40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 20/20: 100%|██████████████████████████████████████████████████████████| 902/902 [05:35<00:00,  2.69it/s]\n",
      "Validation Epoch 20/20: 100%|████████████████████████████████████████████████████████| 226/226 [01:20<00:00,  2.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [20/20], Loss: 0.1026, Accuracy: 0.9669, Val Loss: 0.0619, Val Accuracy: 0.9830\n",
      "Validation loss decreased (0.062538 --> 0.061878).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_model(model, num_epochs, train_loader, valid_loader, optimizer, criterion, accuracy, device, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ef3b8b7a-7b40-4600-a239-e5f9a1de310a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "def predict_image_class(model, image, transform, class_names):\n",
    "    model.eval()\n",
    "    image = transform(image).unsqueeze(0)\n",
    "    image = image.to(device)\n",
    "    with torch.no_grad():\n",
    "        output = model(image)\n",
    "        _, predicted = torch.max(output, 1)\n",
    "    return class_names[predicted.item()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3c5448f5-2763-409f-9792-4f4da6268403",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_hand(image, hands, padding=20, target_size=(300, 300), bg_color=(128, 128, 128)):\n",
    "    rgb_image = image\n",
    "    rgb_image.flags.writeable = False\n",
    "\n",
    "    results = hands.process(rgb_image)\n",
    "\n",
    "    rgb_image.flags.writeable = True\n",
    "\n",
    "    if results.multi_hand_landmarks:\n",
    "        for hand_landmarks in results.multi_hand_landmarks:\n",
    "            h, w, _ = image.shape\n",
    "            x_min, y_min = w, h\n",
    "            x_max, y_max = 0, 0\n",
    "\n",
    "            landmarks = []\n",
    "\n",
    "            for landmark in hand_landmarks.landmark:\n",
    "                x, y = int(landmark.x * w), int(landmark.y * h)\n",
    "                x_min = min(x_min, x)\n",
    "                y_min = min(y_min, y)\n",
    "                x_max = max(x_max, x)\n",
    "                y_max = max(y_max, y)\n",
    "                landmarks.append((x, y, landmark.z))\n",
    "\n",
    "            x_min = max(0, x_min - padding)\n",
    "            y_min = max(0, y_min - padding)\n",
    "            x_max = min(w, x_max + padding)\n",
    "            y_max = min(h, y_max + padding)\n",
    "\n",
    "            hand_image = image[y_min:y_max, x_min:x_max]\n",
    "\n",
    "            hand_h, hand_w, _ = hand_image.shape\n",
    "            scale = min(target_size[0] / hand_w, target_size[1] / hand_h)\n",
    "            new_w = int(hand_w * scale)\n",
    "            new_h = int(hand_h * scale)\n",
    "            resized_hand_image = cv2.resize(hand_image, (new_w, new_h))\n",
    "\n",
    "            result_image = np.full((target_size[1], target_size[0], 3), bg_color, dtype=np.uint8)\n",
    "            x_offset = (target_size[0] - new_w) // 2\n",
    "            y_offset = (target_size[1] - new_h) // 2\n",
    "            result_image[y_offset:y_offset + new_h, x_offset:x_offset + new_w] = resized_hand_image\n",
    "\n",
    "            return result_image, results, (x_min, y_min, scale, x_offset, y_offset), landmarks\n",
    "    return None, None, None, None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dfa899e4-5304-435e-83b9-377f0eef66bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "hands = mp.solutions.hands.Hands(max_num_hands=1)\n",
    "draw = mp.solutions.drawing_utils\n",
    "\n",
    "class_names = dataset.classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4fef55df-3a64-4725-aae7-4c69410be2f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def start_camera(model, hands):\n",
    "    total_images = 0\n",
    "    total_time = 0\n",
    "    while True:\n",
    "        success, image = cap.read()\n",
    "        image = cv2.flip(image, 1)\n",
    "        imageRGB = image\n",
    "\n",
    "        hand_pattern, results, bbox_params, original_landmarks = detect_hand(image, hands)\n",
    "        if results:\n",
    "            for handLms in results.multi_hand_landmarks:\n",
    "                draw.draw_landmarks(image, handLms, mp.solutions.hands.HAND_CONNECTIONS)\n",
    "\n",
    "                x = int(handLms.landmark[0].x * image.shape[1])\n",
    "                y = int(handLms.landmark[0].y * image.shape[0])\n",
    "        \n",
    "        if hand_pattern is not None:\n",
    "            imageRGB = cv2.resize(hand_pattern, (150, 150))\n",
    "        else:\n",
    "            imageRGB = cv2.resize(imageRGB, (150, 150))\n",
    "    \n",
    "        if results:\n",
    "            if results.multi_hand_landmarks:\n",
    "                for handLms in results.multi_hand_landmarks:\n",
    "                    draw.draw_landmarks(image, handLms, mp.solutions.hands.HAND_CONNECTIONS)\n",
    "                    pil_image = Image.fromarray(imageRGB)\n",
    "                    time_s = time.time()\n",
    "                    predicted_class = predict_image_class(model, pil_image, transform, class_names)\n",
    "                    total_time += time.time() - time_s\n",
    "                    total_images += 1\n",
    "                    cv2.putText(image, predicted_class, (x, y), cv2.FONT_HERSHEY_SIMPLEX, 1, (224, 255, 255), 2, cv2.LINE_AA)\n",
    "                    dynamic_console_output(f\"Predicted class: {predicted_class}\")\n",
    "\n",
    "        image_out = image\n",
    "\n",
    "        height, width, _ = imageRGB.shape\n",
    "        height_out, width_out, _ = image_out.shape\n",
    "    \n",
    "        x_offset = width_out - width\n",
    "        y_offset = 0\n",
    "        \n",
    "        image_out[y_offset:y_offset+height, x_offset:x_offset+width] = imageRGB\n",
    "\n",
    "        cv2.imshow('Hand', image_out)\n",
    "        \n",
    "        if cv2.waitKey(33) != -1:\n",
    "            cv2.destroyAllWindows()\n",
    "            # break\n",
    "            return total_time / total_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "96c228cd-5e22-4292-afbc-a59024ddd89f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dynamic_console_output(log):\n",
    "    terminal_width = shutil.get_terminal_size().columns\n",
    "    log = log.ljust(terminal_width)\n",
    "    sys.stdout.write(\"\\r\\033[K\" + log)\n",
    "    sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ab3b1964-9e33-49ae-b09d-2b2b7daf1407",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "97d4f85a-96f1-49bd-8d97-4b02b53f033b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resnet50_t = resnet50\n",
    "resnet50_t.load_state_dict(torch.load('models/resnet50_20.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a6e24724-958f-4ea4-939a-ec05ba109e42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vgg16_t = vgg16\n",
    "vgg16_t.load_state_dict(torch.load('models/vgg16_20.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ee904b31-4a19-4141-9b53-f08b208e2067",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mobilenet_t = mobilenet\n",
    "mobilenet_t.load_state_dict(torch.load('models/mobilenet_20.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "d3c3dadb-3758-4b07-b905-c2a900f30da3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[KPredicted class: S                                                                                                      "
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.027346482331102543"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_camera(resnet50_t, hands)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "53711476-ec3c-45a1-be44-545828ce4bc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[KPredicted class: M                                                                                                      "
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.06815091768900554"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_camera(vgg16_t, hands)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "2d960f94-21c5-4ffa-8660-32f3ba273fc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[KPredicted class: E                                                                                                      "
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.013859311944430637"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_camera(mobilenet_t, hands)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "83e9c79e-0a26-47a2-9994-412c81a987a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "cap.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "52ca53a3-546e-4c10-b3c3-fb9f45137d24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_images_in_folder(folder_path, model):\n",
    "    predictions = []\n",
    "    true_labels = []\n",
    "    \n",
    "    subfolders = [f.path for f in os.scandir(folder_path) if f.is_dir()]\n",
    "    \n",
    "    for subfolder in subfolders:\n",
    "        class_name = os.path.basename(subfolder)\n",
    "        print(f'Processing images in class: {class_name}')\n",
    "     \n",
    "        files = [f for f in os.listdir(subfolder) if f.endswith('.jpg') or f.endswith('.png')]\n",
    "        \n",
    "        for file in files:\n",
    "            image_path = os.path.join(subfolder, file)\n",
    "            image = cv2.imread(image_path)\n",
    "\n",
    "            hand_pattern, results_pattern, bbox_params, original_landmarks = detect_hand(image, hands)\n",
    "            if hand_pattern is not None:\n",
    "                imageRGB = cv2.resize(hand_pattern, (SIZE, SIZE))\n",
    "            else:\n",
    "                imageRGB = cv2.resize(image, (SIZE, SIZE))\n",
    "            if results_pattern:\n",
    "                if results_pattern.multi_hand_landmarks:\n",
    "                    pil_image = Image.fromarray(imageRGB)\n",
    "                    predicted_class = predict_image_class(model, pil_image, transform, class_names)\n",
    "                else:\n",
    "                    predicted_class = 'nothing'\n",
    "            else:\n",
    "                predicted_class = 'nothing'\n",
    "            predictions.append(predicted_class)\n",
    "            true_labels.append(class_name)\n",
    "            \n",
    "    accuracy = accuracy_score(true_labels, predictions)\n",
    "    print(f'Accuracy: {accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "bf437886-7efd-4ae8-ad88-d2fab4b92325",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing images in class: A\n",
      "Processing images in class: B\n",
      "Processing images in class: C\n",
      "Processing images in class: D\n",
      "Processing images in class: del\n",
      "Processing images in class: E\n",
      "Processing images in class: F\n",
      "Processing images in class: G\n",
      "Processing images in class: H\n",
      "Processing images in class: I\n",
      "Processing images in class: J\n",
      "Processing images in class: K\n",
      "Processing images in class: L\n",
      "Processing images in class: M\n",
      "Processing images in class: N\n",
      "Processing images in class: nothing\n",
      "Processing images in class: O\n",
      "Processing images in class: P\n",
      "Processing images in class: Q\n",
      "Processing images in class: R\n",
      "Processing images in class: S\n",
      "Processing images in class: space\n",
      "Processing images in class: T\n",
      "Processing images in class: U\n",
      "Processing images in class: V\n",
      "Processing images in class: W\n",
      "Processing images in class: X\n",
      "Processing images in class: Y\n",
      "Processing images in class: Z\n",
      "Accuracy: 0.2824137931034483\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "path = 'data/test_images'\n",
    "predict_images_in_folder(path, mobilenet_t)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
